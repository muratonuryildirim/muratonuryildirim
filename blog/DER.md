# [DER: Dynamically Expandable Representation for Class Incremental Learning](https://openaccess.thecvf.com/content/CVPR2021/html/Yan_DER_Dynamically_Expandable_Representation_for_Class_Incremental_Learning_CVPR_2021_paper.html)

*Shipeng Yan, Jiangwei Xie, Xuming He.* *CVPR 2021*

**TL;DR:** DER uses a two-stage strategy for incremental learning: 
In stage one, it learns a "super-feature" representation to adapt to new classes while preserving old knowledge. 
In stage two, it combines all features coming from different tasks and classes together on a balanced training subset and fine-tunes the classifier to mitigate the problem of class imbalance.

To handle the task of predicting class labels for both the old and new classes, DER builds a modular deep network. 
This network is composed of two main components: the super-feature extractor network and the linear classifier.

The super-feature extractor network consists of multiple feature extractors, with one extractor for each incremental step.
At each new step, when a new class is added, it expands the super-feature extractor network with a new feature extractor while keeping the parameters of the previous extractors frozen to overcome forgetting. 

Then, features generated by all the extractors are concatenated together and fed into the linear classifier. 
The classifier is responsible for predicting the class labels for the input data based on the combined features.

DER utilizes 3 different losses:

- To enable the new feature extractor to learn diverse and discriminative features for the new classes, an **Auxiliary Loss**.
- To learn feature-level masks and make the network as efficient as possible, a **Sparsity Loss**.
- To dicriminate between all classes learned so far, a **Cross-Entropy (CE) Loss**.

The first 2 Losses manages learning a compact new feature extractor.
After updating the super-feature extractor with the recently learned features while applying pruning, 
DER freezes the super-feature extractor and fine-tune the classifier on a balanced training subset with CE Loss.
This fine-tuning step helps to mitigate the problem of class imbalance, which often arises in incremental learning scenarios when new classes have fewer samples compared to the previously learned classes. 
By fine-tuning on a balanced subset, it ensures that the classifier can make accurate predictions for both old and new classes.



<p align="center">
  <img src="https://github.com/muratonuryildirim/muratonuryildirim/blob/master/blog/img/der.png?raw=true" width=800>
</p>
